{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Test of hierarchical classification (model 2)\n",
    "  \n",
    "  This model uses four layers. In the first layer, every source is classified hierarchically as Stochastic, Transient or Periodic. Then, every source pass trough the next layers (Stochastic, Transient and Periodic). The final classification is dertermined by multiplying the probabilities of the first layer, with their correspondant probabilites of the Stochastic, Transient and Periodic layers. For instance, the probability of being RRL corresponds to the product of the probability of being periodic (according to the first layer) and the probability of being RRL (according to the periodic layer): \n",
    " \n",
    " $$Prob_{final}(RRL) = Prob_{first \\: layer}(Periodic)* Prob_{periodic \\: layer}(RRL)$$\n",
    " \n",
    " And the probability of being a Blazar is computed as:\n",
    " \n",
    "  $$Prob_{final}(Blazar) = Prob_{first \\: layer}(Stochastic)* Prob_{stochastic \\: layer}(Blazar)$$\n",
    "\n",
    "\n",
    " \n",
    " The final classification is definded by the maximum $Prob_{final}$\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing, model_selection, metrics, ensemble\n",
    "import pickle\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier as RandomForestClassifier\n",
    "from scipy.stats import randint as sp_randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Definition of names for plots and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "morethan5 = False\n",
    "\n",
    "extragalactic = False\n",
    "galactic = False\n",
    "\n",
    "date = '20190925'\n",
    "\n",
    "#names of files with detections, features and labels for the training set (v3)\n",
    "labels_file = '../alerce_trainingset/training_set_v4/dfcrossmatches_prioritized_v4.csv'\n",
    "#detections_file = '../alerce_trainingset/training_set_v3/detections.pkl'\n",
    "#features_file = '../alerce_trainingset/training_set_v3/features.pkl'\n",
    "features_file = '../ZTF_data/features20191119.pkl'\n",
    "#non_detections_file = '../alerce_trainingset/training_set_v3/training_non_det_v3.h5'\n",
    "\n",
    "#features_paps = '../ZTF_data/paps_features_all_with_mean.pkl'\n",
    "\n",
    "class_output = '../ZTF_classifications/classification_unlabelled_set_20191017.csv'\n",
    "\n",
    "#where the RF models are saved\n",
    "if morethan5:\n",
    "    model_first_layer = 'stat_prob_hierRF_model_2/rf_model_2_hierarchical_layer_morethan5gr_'+date\n",
    "    model_periodic_layer = 'stat_prob_hierRF_model_2/rf_model_2_periodic_layer_morethan5gr_'+date\n",
    "    model_transient_layer = 'stat_prob_hierRF_model_2/rf_model_2_transient_layer_morethan5gr_'+date\n",
    "    model_stochastic_layer = 'stat_prob_hierRF_model_2/rf_model_2_stochastic_layer_morethan5gr_'+date\n",
    "\n",
    "else: \n",
    "    model_first_layer = 'stat_prob_hierRF_model_2/rf_model_2_hierarchical_layer_'+date\n",
    "    model_periodic_layer = 'stat_prob_hierRF_model_2/rf_model_2_periodic_layer_'+date\n",
    "    model_transient_layer = 'stat_prob_hierRF_model_2/rf_model_2_transient_layer_'+date\n",
    "    model_stochastic_layer = 'stat_prob_hierRF_model_2/rf_model_2_stochastic_layer_'+date\n",
    "\n",
    "#confusion matrixes\n",
    "if morethan5:\n",
    "    conf_matrix_name_first_layer = 'stat_prob_hierRF_model_2/confusion_matrix_rf_model_2_hierarchical_layer_morethan5gr_'+date\n",
    "    conf_matrix_name_second_layer = 'stat_prob_hierRF_model_2/confusion_matrix_rf_model_2_multiclass_morethan5gr_'+date\n",
    "\n",
    "else:\n",
    "    conf_matrix_name_first_layer = 'stat_prob_hierRF_model_2/confusion_matrix_rf_model_2_hierarchical_layer_'+date\n",
    "    conf_matrix_name_second_layer = 'stat_prob_hierRF_model_2/confusion_matrix_rf_model_2_multiclass_'+date\n",
    "\n",
    "\n",
    "#feature importances\n",
    "if morethan5:\n",
    "    feature_importance_name_first_layer = 'stat_prob_hierRF_model_2/feature_importance_rf_model_2_hierarchical_layer_morethan5gr_'+date\n",
    "    feature_importance_name_periodic_layer = 'stat_prob_hierRF_model_2/feature_importance_rf_model_2_periodic_layer_morethan5gr_'+date\n",
    "    feature_importance_name_transient_layer = 'stat_prob_hierRF_model_2/feature_importance_rf_model_2_transient_layer_morethan5gr_'+date\n",
    "    feature_importance_name_stochastic_layer = 'stat_prob_hierRF_model_2/feature_importance_rf_model_2_stochastic_layer_morethan5gr_'+date\n",
    "    \n",
    "else: \n",
    "    feature_importance_name_first_layer = 'stat_prob_hierRF_model_2/feature_importance_rf_model_2_hierarchical_layer_'+date\n",
    "    feature_importance_name_periodic_layer = 'stat_prob_hierRF_model_2/feature_importance_rf_model_2_periodic_layer_'+date\n",
    "    feature_importance_name_transient_layer = 'stat_prob_hierRF_model_2/feature_importance_rf_model_2_transient_layer_'+date\n",
    "    feature_importance_name_stochastic_layer = 'stat_prob_hierRF_model_2/feature_importance_rf_model_2_stochastic_layer_'+date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading the training set files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3057: DtypeWarning: Columns (4,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162996\n"
     ]
    }
   ],
   "source": [
    "#df_nd = pd.read_hdf(non_detections_file)\n",
    "df_feat = pd.read_pickle(features_file)\n",
    "df_labels = pd.read_csv(labels_file,index_col='oid')\n",
    "#df_detections = pd.read_pickle(detections_file)\n",
    "'''\n",
    "df_paps = pd.read_pickle(features_paps)\n",
    "df_paps = df_paps.set_index('oid')\n",
    "df_paps.paps_ratio_1=df_paps.paps_ratio_1.astype(float)\n",
    "df_paps.paps_ratio_2=df_paps.paps_ratio_2.astype(float)\n",
    "df_paps.paps_high_2=df_paps.paps_high_2.astype(float)\n",
    "df_paps.paps_high_1=df_paps.paps_high_1.astype(float)\n",
    "df_paps.paps_low_2=df_paps.paps_low_2.astype(float)\n",
    "df_paps.paps_low_1=df_paps.paps_low_1.astype(float)\n",
    "\n",
    "print(df_paps.dtypes)\n",
    "\n",
    "df_paps = df_paps.replace([np.inf, -np.inf], np.nan)\n",
    "df_paps.replace([-99], -999,inplace=True)\n",
    "df_paps.fillna(-999,inplace=True)\n",
    "print(np.max(df_paps['paps_ratio_2']))\n",
    "\n",
    "'''\n",
    "\n",
    "#deleting columns in df_nd with name ending in \"_3\"\n",
    "#for col in list(df_nd):\n",
    "#    if col[-2:] == \"_3\":\n",
    "#        df_nd.drop(col, inplace=True, axis=1)\n",
    "\n",
    "#getting gscore from the detections file\n",
    "#df_det = df_detections.groupby(['oid'])\n",
    "#df_sgscore = df_det['sgscore1'].median()\n",
    "#df_sgscore = df_sgscore.to_frame()\n",
    "\n",
    "#creating color features\n",
    "#df_nd[\"g_r_max\"] = df_nd.min_mag_fid_1 - df_nd.min_mag_fid_2\n",
    "#df_feat[\"g_r_mean\"] = df_feat.Mean_1 - df_feat.Mean_2\n",
    "\n",
    "#discarging infinite values\n",
    "df_feat = df_feat.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "print(df_labels['class_source'].values.size)\n",
    "#print(df_paps.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162435,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_original</th>\n",
       "      <th>class_hierachical</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ZTF18aawaqeo</th>\n",
       "      <td>CV/Nova</td>\n",
       "      <td>Stochastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZTF18acaxfpa</th>\n",
       "      <td>CV/Nova</td>\n",
       "      <td>Stochastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZTF18abxehba</th>\n",
       "      <td>CV/Nova</td>\n",
       "      <td>Stochastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZTF18aaxjjwu</th>\n",
       "      <td>CV/Nova</td>\n",
       "      <td>Stochastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZTF18abnowur</th>\n",
       "      <td>CV/Nova</td>\n",
       "      <td>Stochastic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             class_original class_hierachical\n",
       "oid                                          \n",
       "ZTF18aawaqeo        CV/Nova        Stochastic\n",
       "ZTF18acaxfpa        CV/Nova        Stochastic\n",
       "ZTF18abxehba        CV/Nova        Stochastic\n",
       "ZTF18aaxjjwu        CV/Nova        Stochastic\n",
       "ZTF18abnowur        CV/Nova        Stochastic"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating new labels to combine SNII and SNIIb classes, and to add RS-CVn as a new class\n",
    "df_labels['class_original'] = df_labels['classALeRCE']\n",
    "df_labels.loc[(df_labels['class_source'] == 'RS CVn'), 'class_original'] = 'RS-CVn'\n",
    "df_labels.loc[(df_labels['class_original'] == 'SNIIn'), 'class_original'] = 'SNII'\n",
    "df_labels.loc[(df_labels['class_original'] == 'AGN-I'), 'class_original'] = 'QSO-I'\n",
    "df_labels.loc[(df_labels['class_source'] == 'A') | (df_labels['class_source'] == 'AGN_galaxy_dominated'), 'class_original'] = 'AGN-I'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#defining the classes included in the RF model\n",
    "label_order = ['QSO-I','AGN-I', 'Blazar', 'CV/Nova', 'SNIa', 'SNIbc', 'SNII',\n",
    "               'SLSN', 'EBSD/D', 'EBC', 'DSCT', 'RRL', 'Ceph', 'LPV','RS-CVn','Periodic-Other']\n",
    "\n",
    "labels = df_labels.loc[df_labels.class_original.isin(label_order)][[\"class_original\"]]\n",
    "\n",
    "#defining hierarchical classes:\n",
    "\n",
    "labels['class_hierachical'] = labels['class_original']\n",
    "\n",
    "labels.loc[ (labels['class_hierachical'] == 'Periodic-Other') | (labels['class_hierachical'] == 'EBSD/D') | (labels['class_hierachical'] == 'EBC')  | (labels['class_hierachical'] == 'DSCT') | (labels['class_hierachical'] == 'RRL') | (labels['class_hierachical'] == 'Ceph') , 'class_hierachical'] = 'Periodic'\n",
    "\n",
    "labels.loc[(labels['class_hierachical'] == 'SNIa') | (labels['class_hierachical'] == 'SNIbc') | (labels['class_hierachical'] == 'SNII') | (labels['class_hierachical'] == 'SLSN'), 'class_hierachical'] = 'Transient'\n",
    "\n",
    "labels.loc[(labels['class_hierachical'] == 'RS-CVn') |  (labels['class_hierachical'] == 'CV/Nova')  |   (labels['class_hierachical'] == 'AGN-I') |  (labels['class_hierachical'] == 'QSO-I') | (labels['class_hierachical'] == 'Blazar')  | (labels['class_hierachical'] == 'LPV') , 'class_hierachical'] = 'Stochastic'\n",
    "\n",
    "cm_classes_hierachical = ['Stochastic','Transient','Periodic']\n",
    "cm_classes_original = label_order\n",
    "\n",
    "print(labels['class_hierachical'].values.shape)\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110911\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_samples_1</th>\n",
       "      <th>Amplitude_1</th>\n",
       "      <th>AndersonDarling_1</th>\n",
       "      <th>Autocor_length_1</th>\n",
       "      <th>Beyond1Std_1</th>\n",
       "      <th>Con_1</th>\n",
       "      <th>Eta_e_1</th>\n",
       "      <th>Gskew_1</th>\n",
       "      <th>MaxSlope_1</th>\n",
       "      <th>Meanvariance_1</th>\n",
       "      <th>...</th>\n",
       "      <th>paps_high_1</th>\n",
       "      <th>paps_high_2</th>\n",
       "      <th>paps_low_1</th>\n",
       "      <th>paps_low_2</th>\n",
       "      <th>paps_ratio_1</th>\n",
       "      <th>paps_ratio_2</th>\n",
       "      <th>positive_fraction_1</th>\n",
       "      <th>positive_fraction_2</th>\n",
       "      <th>rb</th>\n",
       "      <th>sgscore1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ZTF17aaaaact</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.139977</td>\n",
       "      <td>0.328509</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.472415</td>\n",
       "      <td>-0.031614</td>\n",
       "      <td>2.395997</td>\n",
       "      <td>0.004443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>0.004899</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>322.582369</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZTF17aaaaahl</th>\n",
       "      <td>59.0</td>\n",
       "      <td>0.458348</td>\n",
       "      <td>0.956996</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032239</td>\n",
       "      <td>0.532763</td>\n",
       "      <td>3.447957</td>\n",
       "      <td>0.016959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>8.621973e-07</td>\n",
       "      <td>0.526578</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>229.373142</td>\n",
       "      <td>849.637793</td>\n",
       "      <td>0.983051</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.850714</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZTF17aaaaajz</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>4.450488e-05</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.006368</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>143.078357</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.701429</td>\n",
       "      <td>0.979167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZTF17aaaaaly</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.110826</td>\n",
       "      <td>0.427705</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.201831</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.055476</td>\n",
       "      <td>0.003597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004899</td>\n",
       "      <td>6.387781e-06</td>\n",
       "      <td>0.020307</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>4.144941</td>\n",
       "      <td>88.598544</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.684286</td>\n",
       "      <td>0.794833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZTF17aaaaasi</th>\n",
       "      <td>113.0</td>\n",
       "      <td>2.734854</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.433628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002209</td>\n",
       "      <td>0.871545</td>\n",
       "      <td>0.226679</td>\n",
       "      <td>0.107707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>6.273197e-07</td>\n",
       "      <td>0.119522</td>\n",
       "      <td>-0.000414</td>\n",
       "      <td>245.809905</td>\n",
       "      <td>-660.435165</td>\n",
       "      <td>0.539823</td>\n",
       "      <td>0.534351</td>\n",
       "      <td>0.686667</td>\n",
       "      <td>0.902500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              n_samples_1  Amplitude_1  AndersonDarling_1  Autocor_length_1  \\\n",
       "oid                                                                           \n",
       "ZTF17aaaaact         11.0     0.139977           0.328509               1.0   \n",
       "ZTF17aaaaahl         59.0     0.458348           0.956996               6.0   \n",
       "ZTF17aaaaajz          0.0  -999.000000        -999.000000            -999.0   \n",
       "ZTF17aaaaaly         10.0     0.110826           0.427705               2.0   \n",
       "ZTF17aaaaasi        113.0     2.734854           1.000000              18.0   \n",
       "\n",
       "              Beyond1Std_1  Con_1     Eta_e_1     Gskew_1  MaxSlope_1  \\\n",
       "oid                                                                     \n",
       "ZTF17aaaaact      0.363636    0.0    0.472415   -0.031614    2.395997   \n",
       "ZTF17aaaaahl      0.338983    0.0    0.032239    0.532763    3.447957   \n",
       "ZTF17aaaaajz   -999.000000 -999.0 -999.000000 -999.000000 -999.000000   \n",
       "ZTF17aaaaaly      0.300000    0.0    0.201831    0.001717    0.055476   \n",
       "ZTF17aaaaasi      0.433628    0.0    0.002209    0.871545    0.226679   \n",
       "\n",
       "              Meanvariance_1  ...  paps_high_1   paps_high_2  paps_low_1  \\\n",
       "oid                           ...                                          \n",
       "ZTF17aaaaact        0.004443  ...     0.000015 -9.990000e+02    0.004899   \n",
       "ZTF17aaaaahl        0.016959  ...     0.002296  8.621973e-07    0.526578   \n",
       "ZTF17aaaaajz     -999.000000  ...  -999.000000  4.450488e-05 -999.000000   \n",
       "ZTF17aaaaaly        0.003597  ...     0.004899  6.387781e-06    0.020307   \n",
       "ZTF17aaaaasi        0.107707  ...     0.000486  6.273197e-07    0.119522   \n",
       "\n",
       "              paps_low_2  paps_ratio_1  paps_ratio_2  positive_fraction_1  \\\n",
       "oid                                                                         \n",
       "ZTF17aaaaact -999.000000    322.582369   -999.000000             0.000000   \n",
       "ZTF17aaaaahl    0.000733    229.373142    849.637793             0.983051   \n",
       "ZTF17aaaaajz    0.006368   -999.000000    143.078357          -999.000000   \n",
       "ZTF17aaaaaly    0.000566      4.144941     88.598544             1.000000   \n",
       "ZTF17aaaaasi   -0.000414    245.809905   -660.435165             0.539823   \n",
       "\n",
       "              positive_fraction_2          rb    sgscore1  \n",
       "oid                                                        \n",
       "ZTF17aaaaact          -999.000000 -999.000000 -999.000000  \n",
       "ZTF17aaaaahl             0.928571    0.850714    0.500000  \n",
       "ZTF17aaaaajz             0.022727    0.701429    0.979167  \n",
       "ZTF17aaaaaly             0.954545    0.684286    0.794833  \n",
       "ZTF17aaaaasi             0.534351    0.686667    0.902500  \n",
       "\n",
       "[5 rows x 130 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defining columns excluded from the df_nd table\n",
    "\n",
    "rm_nd_cols = [\n",
    "'n_det_fid_1',\n",
    "'n_det_fid_2',\n",
    "'n_pos_1',\n",
    "'n_pos_2',\n",
    "'n_neg_1',\n",
    "'n_neg_2',\n",
    "'paps_non_zero_1',\n",
    "'paps_PN_flag_1',\n",
    "'paps_non_zero_2',\n",
    "'paps_PN_flag_2',\n",
    "]\n",
    "'''\n",
    "'max_mjd_fid_1',\n",
    "'max_mjd_fid_2',\n",
    "'min_mjd_fid_1',\n",
    "'min_mjd_fid_2',\n",
    "'max_mag_fid_1',\n",
    "'max_mag_fid_2',\n",
    "'min_mag_fid_1',\n",
    "'min_mag_fid_2',\n",
    "'first_mag_fid_1',\n",
    "'first_mag_fid_2',\n",
    "'first_mjd_fid_1',\n",
    "'first_mjd_fid_2']\n",
    "'''\n",
    "\n",
    "#print(df_paps.columns)\n",
    "#paps_drop = ['paps_non_zero_1','paps_PN_flag_1','paps_non_zero_2','paps_PN_flag_2']\n",
    "#paps_drop = ['paps_non_zero_1','paps_non_zero_2']\n",
    "\n",
    "#combining all the DF\n",
    "#df = labels.join(df_sgscore).join(df_feat).join(df_nd.drop(rm_nd_cols, axis=1))\n",
    "#df = labels.join(df_sgscore).join(df_feat.drop(rm_nd_cols, axis=1))\n",
    "df = labels.join(df_feat.drop(rm_nd_cols, axis=1),how='inner')#.join(df_paps.drop(paps_drop, axis=1))\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df_train = df.copy()\n",
    "df_train = df_train.fillna(-999)\n",
    "df.drop(['Mean_1','Mean_2','class_original','class_hierachical'], axis=1, inplace=True)\n",
    "df = df.fillna(-999)\n",
    "\n",
    "#print(np.max(df['paps_ratio_2'].values))\n",
    "\n",
    "labels = labels.loc[df.index.values]\n",
    "\n",
    "\n",
    "if extragalactic:\n",
    "    df = df[(df.gal_b > 20) | (df.gal_b < -20)]\n",
    "    labels = labels.loc[df.index.values]\n",
    "\n",
    "if galactic:\n",
    "    df = df[(df.gal_b <= 20) & (df.gal_b >= -20)]\n",
    "    labels = labels.loc[df.index.values]\n",
    "    \n",
    "\n",
    "if morethan5:\n",
    "    df = df[(df.n_samples_1 > 5) & (df.n_samples_2 > 5)]\n",
    "    labels = labels.loc[df.index.values]\n",
    "\n",
    "print(df['g-r_max'].values.size)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Defining functions to plot the confusion matrix and the feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, plot_name,\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "    if morethan5: title='Confusion matrix for more than 5 samples per band'\n",
    "    if extragalactic: title='Confusion matrix for |gal_b|>20'\n",
    "    if galactic: title='Confusion matrix for |gal_b|<=20'\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(plot_name, bbox_inches='tight')\n",
    "    #plt.close()\n",
    "    \n",
    "    \n",
    "def plot_feature_importances(model, feature_names,feature_importances_name):\n",
    "    I = np.argsort(model.feature_importances_)[::-1]\n",
    "    fig, ax = plt.subplots(figsize=(16, 5), tight_layout=True)\n",
    "    x_plot = np.arange(len(model.feature_importances_))\n",
    "    plt.xticks(x_plot, [feature_names[i] for i in I], rotation='vertical')\n",
    "    ax.bar(x_plot, height=model.feature_importances_[I]);\n",
    "    plt.savefig(feature_importances_name)\n",
    "    #plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50621 37373 734\n"
     ]
    }
   ],
   "source": [
    "Y_hierarchical = labels['class_hierachical']#.values\n",
    "Y_original = labels['class_original']#.values\n",
    "\n",
    "X_hierarchical = df#.columns.values.tolist()\n",
    "\n",
    "#splitting training set\n",
    "X_train_hierarchical, X_test_hierarchical, y_train_hierarchical, y_test_hierarchical, y_train_original, y_test_original  = model_selection.train_test_split(X_hierarchical,\n",
    "Y_hierarchical, Y_original, test_size=0.2, stratify=Y_original)\n",
    "\n",
    "\n",
    "# separating training sets for sub-classes\n",
    "X_train_periodic = X_train_hierarchical.loc[y_train_hierarchical=='Periodic', :]\n",
    "#X_train_periodic.drop(['Mean_2'], axis=1,inplace=True)\n",
    "y_train_periodic = y_train_original.loc[y_train_hierarchical=='Periodic']\n",
    "\n",
    "X_train_stochastic = X_train_hierarchical.loc[y_train_hierarchical=='Stochastic', :]\n",
    "y_train_stochastic = y_train_original.loc[y_train_hierarchical=='Stochastic']\n",
    "\n",
    "X_train_transient = X_train_hierarchical.loc[y_train_hierarchical=='Transient', :]\n",
    "#X_train_transient.drop(['Mean_2'], axis=1,inplace=True)\n",
    "y_train_transient = y_train_original.loc[y_train_hierarchical=='Transient']\n",
    "\n",
    "#X_train_hierarchical.drop(['Mean_2'], axis=1,inplace=True)\n",
    "\n",
    "X_test_periodic = X_test_hierarchical#.drop(['Mean_2'], axis=1)\n",
    "X_test_stochastic = X_test_hierarchical#.drop(['Mean_2'], axis=1)\n",
    "X_test_transient = X_test_hierarchical#.drop(['Mean_2'], axis=1)\n",
    "#X_test_hierarchical = X_test_hierarchical#.drop(['Mean_2'], axis=1)\n",
    "\n",
    "\n",
    "print(len(y_train_periodic), len(y_train_stochastic), len(y_train_transient))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Balanced random forest\n",
    "  \n",
    "  ### First layer: separating Periodic, Stochastic and Transients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2f3191d9e70b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodeselektor_hierarchical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrf_model_hierarchical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodeselektor_hierarchical\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_hierarchical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_hierarchical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model selected: \\\"%s\\\"\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodeselektor_hierarchical\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    685\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    664\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 666\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Training first layer of the RF model\n",
    "\n",
    "rf_model_hierarchical = RandomForestClassifier(\n",
    "            n_estimators=500,\n",
    "            max_features='auto',\n",
    "            max_depth=None,\n",
    "            n_jobs=1,\n",
    "            class_weight='balanced_subsample',\n",
    "            criterion='entropy',\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1)\n",
    "\n",
    "#inner_cv = model_selection.KFold(shuffle=True)\n",
    "#outer_cv = model_selection.KFold(shuffle=True)\n",
    "#param_grid = {\"n_estimators\":[400,600], \"max_features\": [0.2,0.4,\"auto\"],\"max_depth\":[None,5,10,15]}\n",
    "param_grid = {\"max_features\": [0.2,\"auto\"],\"max_depth\":[None,15]}\n",
    "modeselektor_hierarchical = model_selection.GridSearchCV(estimator=rf_model_hierarchical, param_grid=param_grid, cv=5, n_jobs=4)\n",
    "\n",
    "modeselektor_hierarchical.fit(X_train_hierarchical, y_train_hierarchical)\n",
    "print(\"Model selected: \\\"%s\\\"\" % modeselektor_hierarchical.best_estimator_)\n",
    "\n",
    "#rf_model_hierarchical.fit(X_train_hierarchical, y_train_hierarchical)\n",
    "\n",
    "#testing first layer performance\n",
    "\n",
    "y_true, y_pred = y_test_hierarchical, modeselektor_hierarchical.predict(X_test_hierarchical)\n",
    "y_pred_proba_hier = modeselektor_hierarchical.predict_proba(X_test_hierarchical)\n",
    "\n",
    "classes_order_proba_hierarchical = modeselektor_hierarchical.classes_\n",
    "print(classes_order_proba_hierarchical)\n",
    "\n",
    "#c = np.c_[y_pred_proba_hier, y_test_original]\n",
    "#c = np.c_[c, y_pred_]\n",
    "#print(c)\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_true, y_pred))\n",
    "print(\"Balanced accuracy:\", metrics.balanced_accuracy_score(y_true, y_pred))\n",
    "\n",
    "#Dumping trained model\n",
    "\n",
    "features_hierarchical = list(X_train_hierarchical)\n",
    "\n",
    "with open(model_first_layer, 'wb') as pickle_file:\n",
    "        model_dump = {\n",
    "            'rf_model': modeselektor_hierarchical,#rf_model_hierarchical,\n",
    "            'features': features_hierarchical,\n",
    "            'order_classes': classes_order_proba_hierarchical\n",
    "            }\n",
    "        pickle.dump(model_dump, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting confusion matrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_true, y_pred, labels=cm_classes_hierachical)\n",
    "print(cnf_matrix)\n",
    "plot_confusion_matrix(cnf_matrix,cm_classes_hierachical,conf_matrix_name_first_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting feature importance\n",
    "plot_feature_importances(modeselektor_hierarchical.best_estimator_, features_hierarchical, feature_importance_name_first_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Periodic layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Periodic layer\n",
    "\n",
    "rf_model_periodic = RandomForestClassifier(\n",
    "            n_estimators=500,\n",
    "            max_features='auto',\n",
    "            max_depth=None,\n",
    "            n_jobs=1,\n",
    "            class_weight='balanced_subsample',\n",
    "            criterion='entropy',\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1)\n",
    "\n",
    "#rf_model_periodic.fit(X_train_periodic, y_train_periodic)\n",
    "\n",
    "\n",
    "#param_grid = {\"n_estimators\":np.arange(200,1000,200), \"max_features\": [0.2,0.4,\"auto\",\"log2\"],\"max_depth\":[None,5,10,15]}\n",
    "modeselektor_periodic = model_selection.GridSearchCV(estimator=rf_model_periodic, param_grid=param_grid, cv=5, n_jobs=4)\n",
    "\n",
    "modeselektor_periodic.fit(X_train_periodic, y_train_periodic)\n",
    "print(\"Model selected: \\\"%s\\\"\" % modeselektor_periodic.best_estimator_)\n",
    "\n",
    "# Applying periodic model to the test data\n",
    "y_true_periodic, y_pred_periodic = y_test_original, modeselektor_periodic.predict(X_test_periodic)\n",
    "y_pred_proba_periodic = modeselektor_periodic.predict_proba(X_test_periodic)\n",
    "\n",
    "classes_order_proba_periodic = modeselektor_periodic.classes_\n",
    "print(classes_order_proba_periodic)\n",
    "\n",
    "#Dumping trained model\n",
    "\n",
    "features_periodic = list(X_train_periodic)\n",
    "\n",
    "with open(model_periodic_layer, 'wb') as pickle_file:\n",
    "        model_dump = {\n",
    "            'rf_model': modeselektor_periodic,\n",
    "            'features': features_periodic,\n",
    "            'order_classes': classes_order_proba_periodic\n",
    "            }\n",
    "        pickle.dump(model_dump, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting feature importance\n",
    "print(len(feature_importance_name_first_layer))\n",
    "plot_feature_importances(modeselektor_periodic.best_estimator_, features_hierarchical, feature_importance_name_first_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Stochastic layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Stochastic layer\n",
    "\n",
    "rf_model_stochastic = RandomForestClassifier(\n",
    "            n_estimators=500,\n",
    "            max_features='auto',\n",
    "            max_depth=None,\n",
    "            n_jobs=1,\n",
    "            class_weight='balanced_subsample',\n",
    "            criterion='entropy',\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1)\n",
    "\n",
    "#rf_model_stochastic.fit(X_train_stochastic, y_train_stochastic)\n",
    "\n",
    "#param_grid = {\"n_estimators\":np.arange(200,1000,200), \"max_features\": [0.2,0.4,\"auto\",\"log2\"],\"max_depth\":[None,5,10,15]}\n",
    "modeselektor_stochastic = model_selection.GridSearchCV(estimator=rf_model_stochastic, param_grid=param_grid, cv=5, n_jobs=4)\n",
    "\n",
    "modeselektor_stochastic.fit(X_train_stochastic, y_train_stochastic)\n",
    "print(\"Model selected: \\\"%s\\\"\" % modeselektor_stochastic.best_estimator_)\n",
    "\n",
    "# Applying stochastic model to the test data\n",
    "y_true_stochastic, y_pred_stochastic  = y_test_original, modeselektor_stochastic.predict(X_test_stochastic)\n",
    "y_pred_proba_stochastic = modeselektor_stochastic.predict_proba(X_test_stochastic)\n",
    "\n",
    "classes_order_proba_stochastic = modeselektor_stochastic.classes_\n",
    "print(classes_order_proba_stochastic)\n",
    "\n",
    "#Dumping trained model\n",
    "\n",
    "features_stochastic = list(X_train_stochastic)\n",
    "\n",
    "with open(model_stochastic_layer, 'wb') as pickle_file:\n",
    "        model_dump = {\n",
    "            'rf_model': modeselektor_stochastic,\n",
    "            'features': features_stochastic,\n",
    "            'order_classes': classes_order_proba_stochastic\n",
    "            }\n",
    "        pickle.dump(model_dump, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting feature importance\n",
    "plot_feature_importances(modeselektor_stochastic.best_estimator_, features_stochastic, feature_importance_name_stochastic_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Transient layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Transient layer\n",
    "\n",
    "rf_model_transient = RandomForestClassifier(\n",
    "            n_estimators=600,\n",
    "            max_features='auto',\n",
    "            max_depth=None,\n",
    "            n_jobs=1,\n",
    "            class_weight='balanced_subsample',\n",
    "            criterion='entropy',\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1)\n",
    "\n",
    "#rf_model_transient.fit(X_train_transient, y_train_transient)\n",
    "\n",
    "#param_grid = {\"n_estimators\":np.arange(200,1000,200), \"max_features\": [0.2,0.4,\"auto\",\"log2\"],\"max_depth\":[None,5,10,15]}\n",
    "modeselektor_transient = model_selection.GridSearchCV(estimator=rf_model_transient, param_grid=param_grid, cv=5, n_jobs=4)\n",
    "\n",
    "modeselektor_transient.fit(X_train_transient, y_train_transient)\n",
    "print(\"Model selected: \\\"%s\\\"\" % modeselektor_transient.best_estimator_)\n",
    "\n",
    "# Applying transient model to the test data\n",
    "y_true_transient, y_pred_transient  = y_test_original, modeselektor_transient.predict(X_test_transient)\n",
    "y_pred_proba_transient = modeselektor_transient.predict_proba(X_test_transient)\n",
    "\n",
    "classes_order_proba_transient = modeselektor_transient.classes_\n",
    "print(classes_order_proba_transient)\n",
    "\n",
    "#Dumping trained model\n",
    "\n",
    "features_transient = list(X_train_transient)\n",
    "\n",
    "with open(model_transient_layer, 'wb') as pickle_file:\n",
    "        model_dump = {\n",
    "            'rf_model': modeselektor_transient,\n",
    "            'features': features_transient,\n",
    "            'order_classes': classes_order_proba_transient\n",
    "            }\n",
    "        pickle.dump(model_dump, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting feature importance\n",
    "plot_feature_importances(modeselektor_transient.best_estimator_, features_transient, feature_importance_name_transient_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Putting al layers together  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating final probabilities\n",
    "\n",
    "#multiplying probabilities of the hierarchical layer with the other layers\n",
    "prob_periodic = y_pred_proba_periodic*y_pred_proba_hier[:,np.where(classes_order_proba_hierarchical=='Periodic')[0][0]].T[:, np.newaxis]\n",
    "prob_stochastic = y_pred_proba_stochastic*y_pred_proba_hier[:,np.where(classes_order_proba_hierarchical=='Stochastic')[0][0]].T[:, np.newaxis]\n",
    "prob_trainsient = y_pred_proba_transient*y_pred_proba_hier[:,np.where(classes_order_proba_hierarchical=='Transient')[0][0]].T[:, np.newaxis]\n",
    "\n",
    "#obtaining final probabilities matrix\n",
    "prob_final = np.concatenate((prob_stochastic,prob_trainsient,prob_periodic),axis=1)\n",
    "\n",
    "print(np.sum(prob_final,axis=1),np.mean(np.sum(prob_final,axis=1)),np.std(np.sum(prob_final,axis=1)))\n",
    "\n",
    "#getting the ordered name of classes for prob_final\n",
    "prob_final_class_names = np.concatenate((classes_order_proba_stochastic,classes_order_proba_transient,classes_order_proba_periodic))\n",
    "print(prob_final_class_names)\n",
    "\n",
    "\n",
    "class_final_proba = np.amax(prob_final,axis=1)\n",
    "class_final_index = np.argmax(prob_final,axis=1)\n",
    "class_final_name = [prob_final_class_names[x] for x in class_final_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating confusion matrix for multilabels\n",
    "cnf_matrix = metrics.confusion_matrix(y_test_original, class_final_name,labels=label_order)\n",
    "print(cnf_matrix)\n",
    "plot_confusion_matrix(cnf_matrix,label_order, conf_matrix_name_second_layer)\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test_original, class_final_name))\n",
    "print(\"Balanced accuracy:\", metrics.balanced_accuracy_score(y_test_original, class_final_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle score\n",
    "\n",
    "num_y_test =  [np.where(prob_final_class_names==x)[0][0] for x in y_test_original] #label_encoder.transform(y_test)\n",
    "\n",
    "#print(num_y_test)\n",
    "\n",
    "CLASSES_REDUCED_V2 = prob_final_class_names\n",
    "\n",
    "def kaggle_loss(labels, predictions, weights=None):\n",
    "    np.clip(predictions, 10**-15, 1-10**-15, out=predictions)\n",
    "    classes = np.unique(labels)\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(classes), dtype=np.float64)/len(classes)\n",
    "    loss_sum = 0\n",
    "    labels = np.array(labels)\n",
    "    for i in classes:\n",
    "        p = predictions[labels == i, i]\n",
    "        class_score = np.mean(np.log(p))*weights[i]\n",
    "        print(CLASSES_REDUCED_V2[i], class_score)\n",
    "        loss_sum += class_score\n",
    "    return -loss_sum/sum(weights)\n",
    "\n",
    "\n",
    "\n",
    "print(kaggle_loss(num_y_test,prob_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Analysis post classification in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oids_test = y_test_original.copy()#.index.values\n",
    "oids_test = oids_test.to_frame()\n",
    "classified_names = np.array(class_final_name)\n",
    "\n",
    "oids_test['pred_class'] = classified_names\n",
    "\n",
    "print(oids_test.head())\n",
    "\n",
    "LPV_asAGN = oids_test.loc[(oids_test.class_original=='AGN-I') & (oids_test.pred_class=='QSO-I')].index.values\n",
    "#LPV_asAGN = y_test_original.loc[y_test_original.values=='QSO-I']\n",
    "\n",
    "\n",
    "print(LPV_asAGN) \n",
    "\n",
    "print(df.loc[LPV_asAGN])\n",
    "test_csv = df.loc[LPV_asAGN]\n",
    "test_csv.to_csv('~/Desktop/blazar_class_as_QSO.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import psycopg2\n",
    "import json\n",
    "\n",
    "credentials_file = \"alercereaduser.json\"\n",
    "with open(credentials_file) as jsonfile:\n",
    "    params = json.load(jsonfile)[\"params\"]\n",
    "    \n",
    "\n",
    "conn = psycopg2.connect(dbname=params['dbname'], user=params['user'], host=params['host'], password=params['password'])\n",
    "    \n",
    "    \n",
    "def plotLC(oid, SN_det, SN_nondet):\n",
    "    fig, ax = plt.subplots(figsize = (10, 5))\n",
    "    labels = {1: 'g', 2: 'r'}\n",
    "    colors = {1: 'g', 2: 'r'}\n",
    "    for fid in [1, 2]:\n",
    "        mask = SN_det.fid == fid\n",
    "        if np.sum(mask) > 0:            \n",
    "            ax.errorbar(SN_det[mask].mjd, SN_det[mask].magpsf_corr, \n",
    "                yerr = SN_det[mask].sigmapsf_corr, c = colors[fid], marker = 'o', label = labels[fid])\n",
    "        #mask = (SN_nondet.fid == fid) & (SN_nondet.diffmaglim > -900)\n",
    "        #if np.sum(mask) > 0:            \n",
    "        #    ax.scatter(SN_nondet[mask].mjd, SN_nondet[mask].diffmaglim, c = colors[fid], alpha = 0.5,\n",
    "        #        marker = 'v', label = \"lim.mag. %s\" % labels[fid])\n",
    "    ax.set_title(oid)\n",
    "    ax.set_xlabel(\"MJD\")\n",
    "    ax.set_ylabel(\"Magnitude\")\n",
    "    ax.legend()\n",
    "    ax.set_ylim(ax.get_ylim()[::-1])\n",
    "    \n",
    "    \n",
    "def getSNdata(oid, doplot = False, doNED = False):\n",
    "\n",
    "    # query detections and sort by mjd\n",
    "    query=\"select oid, ra, dec, fid, mjd, magpsf_corr, sigmapsf_corr from detections where oid='%s'\" % oid\n",
    "    SN_det = pd.read_sql_query(query, conn)\n",
    "    SN_det.sort_values(by=['mjd'], inplace=True)\n",
    "        \n",
    "    # query non detections and sort by mjd\n",
    "    query=\"select oid, fid, mjd, diffmaglim from non_detections where oid='%s'\" % oid\n",
    "    SN_nondet = pd.read_sql_query(query, conn)\n",
    "    SN_nondet.sort_values(by=['mjd'], inplace=True)\n",
    "    \n",
    "    if doplot:\n",
    "        plotLC(oid, SN_det, SN_nondet)\n",
    "        \n",
    "    # find NED galaxies\n",
    "    if doNED:\n",
    "        co = coordinates.SkyCoord(ra=SNe.meanra[oid], dec=SNe.meandec[oid], unit=(u.deg, u.deg), frame='fk4')\n",
    "        result_table = Ned.query_region(co, radius=0.01 * u.deg, equinox='J2000.0')\n",
    "        display(result_table)\n",
    "                \n",
    "    # return data\n",
    "    return SN_det, SN_nondet\n",
    "\n",
    "\n",
    "for oid in LPV_asAGN:\n",
    "    getSNdata(oid, doplot = True, doNED = False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Classifying unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    "\n",
    "print(df_feat.n_samples_1.size)\n",
    "\n",
    "#rm_nd_cols = ['n_det_fid_1', 'n_det_fid_2', 'n_pos_1', 'n_pos_2', 'n_neg_1', 'n_neg_2',\n",
    "#             'Mean_1','Mean_2','paps_non_zero_1','paps_non_zero_2']\n",
    "\n",
    "\n",
    "rm_nd_cols = [\n",
    "'n_det_fid_1',\n",
    "'n_det_fid_2',\n",
    "'Mean_1',\n",
    "'Mean_2',\n",
    "'n_pos_1',\n",
    "'n_pos_2',\n",
    "'n_neg_1',\n",
    "'n_neg_2',\n",
    "'paps_non_zero_1',\n",
    "'paps_PN_flag_1',\n",
    "'paps_non_zero_2',\n",
    "'paps_PN_flag_2',\n",
    "]\n",
    "\n",
    "df_feat_ul = df_feat.drop(rm_nd_cols, axis=1)\n",
    "#df_feat_ul = df_feat_ul.join(df_paps.drop(paps_drop, axis=1))\n",
    "df_feat_ul.fillna(-999,inplace=True)\n",
    "\n",
    "print(df_feat_ul.n_samples_1.size)\n",
    "\n",
    "if extragalactic:\n",
    "    df_feat_ul = df_feat_ul[(df_feat_ul.gal_b > 20) | (df_feat_ul.gal_b < -20)]\n",
    "\n",
    "if galactic:\n",
    "    df_feat_ul = df_feat_ul[(df_feat_ul.gal_b <= 20) & (df_feat_ul.gal_b >= -20)]\n",
    "    \n",
    "if morethan5:\n",
    "    df_feat_ul = df_feat_ul[(df_feat_ul.n_samples_1 > 5) & (df_feat_ul.n_samples_2 > 5)]\n",
    "    \n",
    "df_feat_ul_out = df_feat_ul\n",
    "    \n",
    "#df_feat_ul_stochastic = df_feat_ul\n",
    "\n",
    "#df_feat_ul = df_feat_ul.drop(['Mean_2'], axis=1)\n",
    "\n",
    "#df_feat_ul_stochastic.head()\n",
    "print(df_feat_ul.n_samples_1.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting classes of unlabeled data\n",
    "\n",
    "\n",
    "test_Y_hierarchical = modeselektor_hierarchical.predict(df_feat_ul)\n",
    "test_Y_proba_hierarchical = modeselektor_hierarchical.predict_proba(df_feat_ul)\n",
    "\n",
    "test_Y_periodic = modeselektor_periodic.predict(df_feat_ul)\n",
    "test_Y_proba_periodic = modeselektor_periodic.predict_proba(df_feat_ul)\n",
    "\n",
    "test_Y_stochastic = modeselektor_stochastic.predict(df_feat_ul)\n",
    "test_Y_proba_stochastic = modeselektor_stochastic.predict_proba(df_feat_ul)\n",
    "\n",
    "test_Y_transient = modeselektor_transient.predict(df_feat_ul)\n",
    "test_Y_proba_transient = modeselektor_transient.predict_proba(df_feat_ul)\n",
    "\n",
    "\n",
    "#multiplying probabilities\n",
    "prob_periodic_ul = test_Y_proba_periodic*test_Y_proba_hierarchical[:,np.where(classes_order_proba_hierarchical=='Periodic')[0][0]].T[:, np.newaxis]\n",
    "prob_stochastic_ul = test_Y_proba_stochastic*test_Y_proba_hierarchical[:,np.where(classes_order_proba_hierarchical=='Stochastic')[0][0]].T[:, np.newaxis]\n",
    "prob_trainsient_ul = test_Y_proba_transient*test_Y_proba_hierarchical[:,np.where(classes_order_proba_hierarchical=='Transient')[0][0]].T[:, np.newaxis]\n",
    "\n",
    "#obtaining final probabilities matrix\n",
    "prob_final_ul = np.concatenate((prob_stochastic_ul,prob_trainsient_ul,prob_periodic_ul),axis=1)\n",
    "\n",
    "print(np.sum(prob_final_ul,axis=1),np.mean(np.sum(prob_final_ul,axis=1)),np.std(np.sum(prob_final_ul,axis=1)))\n",
    "\n",
    "#getting the ordered name of classes for prob_final\n",
    "prob_final_class_names_ul = np.concatenate((classes_order_proba_stochastic,classes_order_proba_transient,classes_order_proba_periodic))\n",
    "print(prob_final_class_names)\n",
    "\n",
    "\n",
    "class_final_proba_ul = np.amax(prob_final_ul,axis=1)\n",
    "class_final_index_ul = np.argmax(prob_final_ul,axis=1)\n",
    "class_final_name_ul = [prob_final_class_names_ul[x] for x in class_final_index_ul]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing results in the output\n",
    "\n",
    "df_out = df_feat_ul_out\n",
    "print(df_out.shape)\n",
    "print(len(class_final_name_ul))\n",
    "print(len(prob_final_ul))\n",
    "\n",
    "\n",
    "df_out['predicted_class'] = class_final_name_ul\n",
    "df_out['predicted_class_proba'] = class_final_proba_ul\n",
    "test_data_withclass = df_out\n",
    "\n",
    "'''\n",
    "probs_header = prob_final_class_names_ul + '_prob'\n",
    "\n",
    "prob_pd_ul = pd.DataFrame(prob_final_ul,columns=probs_header,index=df_out.index)\n",
    "\n",
    "prob_h_pd_ul = pd.DataFrame(test_Y_proba_hierarchical, columns=['prob_Periodic','prob_Stochastic','prob_Transient'],index=df_out.index)\n",
    "\n",
    "test_data_withclass = df_out.join(prob_pd_ul).join(prob_h_pd_ul)\n",
    "\n",
    "test_data_withclass.to_csv(class_output)\n",
    "\n",
    "test_data_withclass.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_withclass.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_counts = Counter(class_final_name_ul)\n",
    "df_hist = pd.DataFrame.from_dict(letter_counts, orient='index')\n",
    "df_hist.plot(kind='bar')\n",
    "#plt.yscale('log')\n",
    "plt.title('sources classified')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(nrows = len(prob_final_class_names_ul), figsize=(12, 20), sharex=True)\n",
    "for idx, cl in enumerate(prob_final_class_names_ul):\n",
    "   #print(idx, cl)\n",
    "   print(idx, cl, np.percentile(test_data_withclass['predicted_class_proba'][test_data_withclass['predicted_class']==cl].values, 50.))\n",
    "   ax[idx].axvline(np.percentile(test_data_withclass['predicted_class_proba'][test_data_withclass['predicted_class']==cl].values, 5.), c='g')\n",
    "   ax[idx].axvline(np.percentile(test_data_withclass['predicted_class_proba'][test_data_withclass['predicted_class']==cl].values, 50.), c='r')\n",
    "   ax[idx].axvline(np.percentile(test_data_withclass['predicted_class_proba'][test_data_withclass['predicted_class']==cl].values, 95.), c='g')\n",
    "   ax[idx].axvline(1/len(prob_final_class_names_ul), c = 'k')\n",
    "   ax[idx].hist(test_data_withclass['predicted_class_proba'][test_data_withclass['predicted_class']==cl].values, density=True, bins=40, lw=5, label=cl, alpha=0.8)\n",
    "   ax[idx].text(test_data_withclass['predicted_class_proba'][test_data_withclass['predicted_class']==cl].max(), 1., \" %s (%i)\" % (cl, test_data_withclass['predicted_class_proba'][test_data_withclass['predicted_class']==cl].shape[0]), fontsize=18, va='bottom', ha='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'g-r_max'\n",
    "logscale = False\n",
    "df_train_plot = df_train#.join(labels)\n",
    "df_train_plot = df_train_plot.loc[df_train_plot[feature]>-999] \n",
    "fig, ax = plt.subplots(nrows = len(label_order), figsize=(12, 20), sharex=True)\n",
    "\n",
    "for idx, cl in enumerate(label_order):\n",
    "   #print(idx, cl)\n",
    "   print(idx, cl, np.percentile(df_train_plot[feature][df_train_plot['class_original']==cl].values, 50.))\n",
    "   ax[idx].axvline(np.percentile(df_train_plot[feature][df_train_plot['class_original']==cl].values, 5.), c='g')\n",
    "   ax[idx].axvline(np.percentile(df_train_plot[feature][df_train_plot['class_original']==cl].values, 50.), c='r')\n",
    "   ax[idx].axvline(np.percentile(df_train_plot[feature][df_train_plot['class_original']==cl].values, 95.), c='g')\n",
    "   ax[idx].axvline(1/len(prob_final_class_names_ul), c = 'k')\n",
    "   if logscale: \n",
    "      feat = np.log10(df_train_plot[feature][df_train_plot['class_original']==cl].values)\n",
    "      feat = feat[feat>0]\n",
    "      ax[idx].hist(np.log10(df_train_plot[feature][df_train_plot['class_original']==cl].values), density=False, bins=30, lw=5, label=cl, alpha=0.8)\n",
    "   else: ax[idx].hist(df_train_plot[feature][df_train_plot['class_original']==cl].values, density=False, bins=30, lw=5, label=cl, alpha=0.8)\n",
    "   ax[idx].text(df_train_plot[feature][df_train_plot['class_original']==cl].max(), 1.5, \" %s (%i)\" % (cl, df_train_plot[feature][df_train_plot['class_original']==cl].shape[0]), fontsize=18, va='bottom', ha='left')\n",
    "\n",
    "#plt.savefig('../feature_analysis_train_set/'+feature+'.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "feature_x = 'g-r_max'\n",
    "feature_y = 'gal_b'\n",
    "#labels_list = ['RS-CVn','Ceph','LPV','RRL','AGN-I','Blazar','SNIa']\n",
    "labels_list = ['QSO-I','AGN-I']#,'Blazar']\n",
    "logscale = False\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(labels_list)))\n",
    "#df_train = df_train.join(labels)\n",
    "df_test = test_data_withclass.loc[(test_data_withclass[feature_y]>-999) & (test_data_withclass[feature_x]>-999)] \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "#ax.set_xlim(-2,6)\n",
    "\n",
    "for idx, cl in enumerate(labels_list):\n",
    "   #print(idx, cl)\n",
    "   ax.scatter(df_test[feature_x][df_test['predicted_class']==cl].values,df_test[feature_y][df_test['predicted_class']==cl].values,color=colors[idx],alpha = 0.5, s = 3,label=cl)\n",
    "   \n",
    "plt.legend()\n",
    "plt.savefig('../feature_analysis_train_set/color_gal_b_selection.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "feature_x = 'g-r_max'#'GP_DRW_sigma_1'\n",
    "feature_y = 'gal_b' #'GP_DRW_tau_1'\n",
    "labels_list = ['CV/Nova','QSO-I','AGN-I','Blazar']\n",
    "logscale = False\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(labels_list)))\n",
    "\n",
    "df_plot = df.join(labels)\n",
    "df_plot= df_plot.loc[(df_plot[feature_y]>-999) & (df_plot[feature_x]>-999)] \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "ax.set_xlim(-2,6)\n",
    "#ax.set_xlim(-4,3)\n",
    "#ax.set_ylim(-4,5)\n",
    "for idx, cl in enumerate(labels_list):\n",
    "   #print(idx, cl)\n",
    "   ax.scatter(df_plot[feature_x][df_plot['class_original']==cl].values,df_plot[feature_y][df_plot['class_original']==cl].values,color=colors[idx],alpha=0.6,label=cl)\n",
    "   #ax.scatter(np.log10(df_plot[feature_x][df_plot['class_original']==cl].values),np.log10(df_plot[feature_y][df_plot['class_original']==cl].values),color=colors[idx],alpha=0.6,label=cl)\n",
    "   \n",
    "plt.legend()\n",
    "#plt.savefig('../feature_analysis_train_set/'+feature+'.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_withclass = test_data_withclass.loc[test_data_withclass.rb>0.67]\n",
    "fig, ax = plt.subplots(nrows = len(prob_final_class_names_ul), figsize=(12, 20), sharex=True)\n",
    "for idx, cl in enumerate(prob_final_class_names_ul):\n",
    "   #print(idx, cl)\n",
    "   print(idx, cl, np.percentile(test_data_withclass['predicted_class_proba'][test_data_withclass['predicted_class']==cl].values, 50.))\n",
    "   ax[idx].axvline(np.percentile(test_data_withclass['predicted_class_proba'][test_data_withclass['predicted_class']==cl].values, 5.), c='g')\n",
    "   ax[idx].axvline(np.percentile(test_data_withclass['predicted_class_proba'][test_data_withclass['predicted_class']==cl].values, 50.), c='r')\n",
    "   ax[idx].axvline(np.percentile(test_data_withclass['predicted_class_proba'][test_data_withclass['predicted_class']==cl].values, 95.), c='g')\n",
    "   ax[idx].axvline(1/len(prob_final_class_names_ul), c = 'k')\n",
    "   ax[idx].hist(test_data_withclass['predicted_class_proba'][test_data_withclass['predicted_class']==cl].values, density=True, bins=40, lw=5, label=cl, alpha=0.8)\n",
    "   ax[idx].text(test_data_withclass['predicted_class_proba'][test_data_withclass['predicted_class']==cl].max(), 1., \" %s (%i)\" % (cl, test_data_withclass['predicted_class_proba'][test_data_withclass['predicted_class']==cl].shape[0]), fontsize=18, va='bottom', ha='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_x = 'g-r_max'\n",
    "feature_y = 'gal_b'\n",
    "#labels_list = ['RS-CVn','Ceph','LPV','RRL','AGN-I','Blazar','SNIa']\n",
    "labels_list = ['QSO-I','AGN-I','Blazar']\n",
    "logscale = False\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(labels_list)))\n",
    "#df_train = df_train.join(labels)\n",
    "df_test = test_data_withclass.loc[(test_data_withclass[feature_y]>-999) & (test_data_withclass[feature_x]>-999)] \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "ax.set_xlim(-2,6)\n",
    "\n",
    "for idx, cl in enumerate(labels_list):\n",
    "   #print(idx, cl)\n",
    "   ax.scatter(df_test[feature_x][df_test['predicted_class']==cl].values,df_test[feature_y][df_test['predicted_class']==cl].values,color=colors[idx],alpha = 0.5, s = 3,label=cl)\n",
    "   \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
